{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NOAA HRRR is a real-time 3km resolution, hourly updated, cloud-resolving, convection-allowing atmospheric model, initialized by 3km grids with 3km radar assimilation.\n",
    "\n",
    "This notebook provides an example of accessing HRRR data, including (1) finding the data file corresponding to a date and time, (2) retrieving a portion of that file from blob storage which includes the surface temperature variable, (3) opening the file using the xarray library, and (4) rendering an image of the forecast."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is documented at http://aka.ms/ai4edata-hrrr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import climatedata_functions as climf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"../data/metadata.csv\")\n",
    "ds = climf.get_ds()#only works with recent dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make metadafile with gridpoints\n",
    "\n",
    "In the next cell the grid points are calculated. This needs to be executed only once and then the file is saved and then just loaded if the notebook is executed anther time. (Note: takes 6-8 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranform coordinates for longitude\n",
    "metadata['longitude_trans'] = metadata['longitude']+360 \n",
    "#add columns for gridpoints\n",
    "metadata['x_grid'] = ''\n",
    "metadata['y_grid'] = ''\n",
    "\n",
    "#make new metadatafile and save as cvs \n",
    "#metadata_new = climf.save_grids(metadata, ds) #(takes approx 6 minutes)\n",
    "#metadata_new.to_csv('../data/metadata_grids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load new metadata file that includes the gridpoints\n",
    "metadata = pd.read_csv(\"../data/metadata_grids.csv\")\n",
    "metadata.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get temperatures for gridpoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make function that gets the mean/median temperature for one date for all the places sampled at that date (to save time) and the previos x days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"../data/metadata_temp_12_01_06_complete_without_a.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data list:**\n",
    "\n",
    "**Radiation**\n",
    "* DSWRF:surface\t(Downward Short-Wave Radiation Flux [W/m^2])\n",
    "* DLWRF:surface\t(Downward Long-Wave Rad. Flux [W/m^2])\n",
    "\n",
    "**Wind**\n",
    "* WIND:10 m above ground (0-0 day max f\tWind Speed [m/s])\n",
    "* UGRD:10 m above ground (analysis\tU-Component of Wind [m/s]) --> also avalable as 0-0 day max)\n",
    "* VGRD:10 m above ground (analysis\tV-Component of Wind [m/s]) --> also avalable as 0-0 day max)\n",
    "\n",
    "**Temperature**\n",
    "* maybe think abut getting a different time (UTS vs time at a specific place...)\n",
    "\n",
    "We want to get the model for hours: 6, 12, 18, and 24h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define  download parameters\n",
    "days = 7  #how many days to go back?\n",
    "hour = 18 #which our of the day (UTS time!!!)# we want to test: 6,12,18,24\n",
    "param_layer = ':WIND:10 m above ground' # options: ':TMP:surface', ' \":DSWRF:surface\"'  surface temperature, #available parameters and layes: https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsfcf00.grib2.shtml\n",
    "forecast_param = 'si10'#for temperature: 't', for wind:10maboveground: 'si10', for dswrf: 'dswrf' for name of the column in the metadata and the grib2 data\n",
    "#dswrf = Downward Short-Wave Radiation Flux [W/m^2] https://www.goes-r.gov/products/baseline-DSR.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-14\n",
      "2016-08-31\n",
      "2020-11-19\n",
      "2016-08-24\n",
      "2019-07-23\n",
      "2021-08-23\n",
      "2017-11-15\n",
      "2020-06-10\n",
      "2014-08-12\n",
      "2018-06-27\n",
      "2013-11-06\n",
      "2015-08-24\n",
      "2014-11-01\n",
      "2021-10-18\n",
      "2015-08-26\n",
      "2019-08-26\n",
      "2018-01-08\n",
      "2015-08-18\n",
      "2017-07-28\n",
      "2015-07-07\n",
      "2018-06-13\n",
      "2018-01-09\n",
      "2013-07-09\n",
      "2020-08-13\n",
      "2013-08-17\n",
      "2018-08-13\n",
      "2017-03-09\n"
     ]
    }
   ],
   "source": [
    "#from tqdm import tqdm\n",
    "#pbar.set_description(f\"Processing {start_date} --> {temp_list[0][0]}\")\n",
    "\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "#make empty list to store the dates already sampled\n",
    "done_list = []\n",
    "\n",
    "metadata[forecast_param+'_'+str(hour)] = '' #create new column to store the data \n",
    "#getting the data\n",
    "for idx,row in enumerate(metadata.index): #takes first element in metadata list\n",
    "    #print(row, idx)\n",
    "    start_date = metadata.date[idx]\n",
    "    date_index_list = metadata.index[metadata.date == start_date]#list with all the indexes of dates with the same date\n",
    "    #print(date_index_list)\n",
    "    if start_date not in done_list:#if data for the given date was already downloaded, this row will be skipped\n",
    "        temp_list = [[0] * days for i in range(len(date_index_list))]#make list of lists to store the values inside\n",
    "        done_list.append(start_date)#list of dates already samples\n",
    "        start_date = climf.get_start_date(start_date)#formate to time object\n",
    "        print(start_date)\n",
    "        for x in range(days):\n",
    "            count = 0\n",
    "            #x = x*2 (if we want to take only every second day)\n",
    "            day_date = start_date - timedelta(days=x)\n",
    "            #ds, stop = climf.get_ds_aws_array(day_date, hour,param_layer, forecast_param)#getting the temperature array for the specified date\n",
    "            ds, stop = climf.get_ds_aws_array(day_date, hour,param_layer, forecast_param)#getting the temperature array for the specified date\n",
    "            for index in date_index_list:\n",
    "                x_grid = metadata.x_grid[index]\n",
    "                y_grid = metadata.y_grid[index]\n",
    "                if stop == True:\n",
    "                    temp_list[count][x] = np.nan\n",
    "                else:\n",
    "                    temp_list[count][x] = ds[x_grid][y_grid]\n",
    "                count += 1\n",
    "                if x == days-1 and index == date_index_list[len(date_index_list)-1]:#if condition is met put the values in the metadata file\n",
    "                    for i in range(len(temp_list)):                    \n",
    "                        metadata[forecast_param+'_'+str(hour)].loc[date_index_list[i]] = temp_list[i] #not index but  \n",
    "    else:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.to_csv(f'../data/metadata_{forecast_param}_{str(hour)}_complete.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the download was interupted the code in the next cell can be used to continue with the download. Since the dates already dowloaded are saved in 'done_list', it can continue from that withouth starting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#done_list.pop()#remove last element (in case it didn work with that)\n",
    "#len(done_list)#how many dates are already inside (1637 unique dates in total)\n",
    "done_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "\n",
    "temp_list = []\n",
    "#done_list = [] deactivate because we want to continue from the old donelist\n",
    "days = 7\n",
    "hour = 12 #which our of the day\n",
    "\n",
    "\n",
    "for idx,row in enumerate(metadata.index): #takes first element in metadata list\n",
    "    #print(row, idx)\n",
    "    start_date = metadata.date[idx]\n",
    "    date_index_list = metadata.index[metadata.date == start_date]#list with all the indexes of dates with the same date\n",
    "    #print(date_index_list)\n",
    "    if start_date not in done_list:#only do if this date wasnt used before\n",
    "        temp_list = [[0] * days for i in range(len(date_index_list))]#make list of lists to store the values inside\n",
    "        #print(len(temp_list), len(temp_list[0]))\n",
    "        done_list.append(start_date)#list of dates already samples\n",
    "        start_date = climf.get_start_date(start_date)#formate to time object\n",
    "        print(start_date)\n",
    "        for x in range(days):\n",
    "            count = 0\n",
    "            day_date = start_date - timedelta(days=x)\n",
    "            ds, stop = climf.get_ds_aws_array(day_date, hour,param_layer, forecast_param)#getting the temperature array for the specified date\n",
    "            for index in date_index_list:\n",
    "                x_grid = metadata.x_grid[index]\n",
    "                y_grid = metadata.y_grid[index]\n",
    "                if stop == True:\n",
    "                    temp_list[count][x] = np.nan\n",
    "                else:\n",
    "                    temp_list[count][x] = ds[x_grid][y_grid]\n",
    "                count += 1\n",
    "                if x == days-1 and index == date_index_list[len(date_index_list)-1]:#if condition is met put the values in the metadata file\n",
    "                    for i in range(len(temp_list)):                    \n",
    "                        metadata[forecast_param+'_'+str(hour)].loc[date_index_list[i]] = temp_list[i] #not index but  \n",
    "    else:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata.to_csv(f'../data/metadata_{forecast_param}_{str(hour)}_complete.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad948ff7b48983452e55733fd71e7df6f615a0b1ed05e1cd9f8bab22d77c40aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
