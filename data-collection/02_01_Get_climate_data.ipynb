{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NOAA HRRR is a real-time 3km resolution, hourly updated, cloud-resolving, convection-allowing atmospheric model, initialized by 3km grids with 3km radar assimilation.\n",
    "\n",
    "This notebook provides an example of accessing HRRR data, including (1) finding the data file corresponding to a date and time, (2) retrieving a portion of that file from blob storage which includes the surface temperature variable, (3) opening the file using the xarray library, and (4) rendering an image of the forecast."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is documented at http://aka.ms/ai4edata-hrrr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import climatedata_functions as climf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"../data/metadata.csv\")\n",
    "ds = climf.get_ds()#only works with recent dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make metadafile with gridpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranform coordinates for longitude\n",
    "metadata['longitude_trans'] = metadata['longitude']+360 \n",
    "#add columns for gridpoints\n",
    "metadata['x_grid'] = ''\n",
    "metadata['y_grid'] = ''\n",
    "\n",
    "#make new metadatafile and save as cvs \n",
    "#metadata_new = climf.save_grids(metadata.head(), ds) #(takes approx 6 minutes)\n",
    "#metadata_new.to_csv('../data/metadata_grids.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load new metadata file that includes the gridpoints\n",
    "metadata = pd.read_csv(\"../data/metadata_grids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metadata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metadata\u001b[39m.\u001b[39mhead(\u001b[39m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metadata' is not defined"
     ]
    }
   ],
   "source": [
    "metadata.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get temperatures for gridpoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make function that gets the mean/median temperature for one date for all the places sampled at that date (to save time) and the previos x days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"../data/metadata_temp_12_01_06_complete_without_a.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data list:**\n",
    "\n",
    "**Radiation**\n",
    "* DSWRF:surface\t(Downward Short-Wave Radiation Flux [W/m^2])\n",
    "* DLWRF:surface\t(Downward Long-Wave Rad. Flux [W/m^2])\n",
    "\n",
    "**Wind**\n",
    "* WIND:10 m above ground (0-0 day max f\tWind Speed [m/s])\n",
    "* UGRD:10 m above ground (analysis\tU-Component of Wind [m/s]) --> also avalable as 0-0 day max)\n",
    "* VGRD:10 m above ground (analysis\tV-Component of Wind [m/s]) --> also avalable as 0-0 day max)\n",
    "\n",
    "**Temperature**\n",
    "* maybe think abut getting a different time (UTS vs time at a specific place...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define  download parameters\n",
    "days = 7  #how many days to go back?\n",
    "hour = 12 #which our of the day (UTS time!!!)\n",
    "param_layer = ':DSWRF:surface' # :TMP:surface  surface temperature, #available parameters and layes: https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsfcf00.grib2.shtml\n",
    "forecast_param = 'dswrf'#name of the column in the metadata table\n",
    "#dswrf = Downward Short-Wave Radiation Flux [W/m^2] https://www.goes-r.gov/products/baseline-DSR.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "#make empty list to store the dates already sampled\n",
    "done_list = []\n",
    "\n",
    "metadata[forecast_param+'_'+str(hour)] = '' #create new column to store the data \n",
    "#getting the data\n",
    "for idx,row in enumerate(metadata.index): #takes first element in metadata list\n",
    "    #print(row, idx)\n",
    "    start_date = metadata.date[idx]\n",
    "    date_index_list = metadata.index[metadata.date == start_date]#list with all the indexes of dates with the same date\n",
    "    #print(date_index_list)\n",
    "    if start_date not in done_list:#if data for the given date was already downloaded, this row will be skipped\n",
    "        temp_list = [[0] * days for i in range(len(date_index_list))]#make list of lists to store the values inside\n",
    "        done_list.append(start_date)#list of dates already samples\n",
    "        start_date = climf.get_start_date(start_date)#formate to time object\n",
    "        print(start_date)\n",
    "        for x in range(days):\n",
    "            count = 0\n",
    "            #x = x*2 (if we want to take only every second day)\n",
    "            day_date = start_date - timedelta(days=x)\n",
    "            #ds, stop = climf.get_ds_aws_array(day_date, hour,param_layer, forecast_param)#getting the temperature array for the specified date\n",
    "            ds, stop = climf.get_ds_aws_array(day_date, hour,param_layer, forecast_param)#getting the temperature array for the specified date\n",
    "            print(ds, stop)\n",
    "            for index in date_index_list:\n",
    "                x_grid = metadata.x_grid[index]\n",
    "                y_grid = metadata.y_grid[index]\n",
    "                if stop == True:\n",
    "                    temp_list[count][x] = np.nan\n",
    "                else:\n",
    "                    temp_list[count][x] = ds[x_grid][y_grid]\n",
    "                count += 1\n",
    "                if x == days-1 and index == date_index_list[len(date_index_list)-1]:#if condition is met put the values in the metadata file\n",
    "                    for i in range(len(temp_list)):                    \n",
    "                        metadata[forecast_param+'_'+str(hour)].loc[date_index_list[i]] = temp_list[i] #not index but  \n",
    "    else:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.to_csv('../data/metadata_temp_06_12_dswrf_12_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-14\n",
      "2016-08-31\n",
      "2020-11-19\n",
      "2016-08-24\n",
      "2019-07-23\n",
      "2021-08-23\n",
      "2017-11-15\n",
      "2020-06-10\n",
      "2014-08-12\n",
      "2018-06-27\n",
      "2013-11-06\n",
      "2015-08-24\n",
      "2014-11-01\n",
      "2021-10-18\n",
      "2015-08-26\n",
      "2019-08-26\n",
      "2018-01-08\n",
      "2015-08-18\n",
      "2017-07-28\n",
      "2015-07-07\n",
      "2018-06-13\n",
      "2018-01-09\n",
      "2013-07-09\n",
      "2020-08-13\n",
      "2013-08-17\n",
      "2018-08-13\n",
      "2017-03-09\n",
      "2013-07-29\n",
      "2017-12-05\n",
      "2019-06-06\n",
      "2020-02-26\n",
      "2018-04-11\n",
      "2020-05-18\n",
      "2016-07-20\n",
      "2018-08-27\n",
      "2020-03-18\n",
      "2019-01-17\n",
      "2016-06-21\n",
      "2015-01-13\n",
      "2016-06-08\n",
      "2016-03-08\n",
      "2013-01-15\n",
      "2021-07-14\n",
      "2017-06-19\n",
      "2016-04-13\n",
      "2019-07-02\n",
      "2019-05-22\n",
      "2014-02-28\n",
      "2014-07-15\n",
      "2017-08-01\n",
      "2019-09-24\n",
      "2017-09-26\n",
      "2016-01-19\n",
      "2013-08-20\n",
      "2017-08-02\n",
      "2016-06-22\n",
      "2015-06-10\n",
      "2018-04-03\n",
      "2016-04-06\n",
      "2016-05-17\n",
      "2017-10-05\n",
      "2013-07-16\n",
      "2016-07-19\n",
      "2014-12-03\n",
      "2016-08-05\n",
      "2020-05-12\n",
      "2015-09-25\n",
      "2017-06-12\n",
      "2017-03-31\n",
      "2019-03-19\n",
      "2017-08-28\n",
      "2015-04-22\n",
      "2019-08-22\n",
      "2016-05-24\n",
      "2016-12-13\n",
      "2015-04-14\n",
      "2020-06-24\n",
      "2014-07-22\n",
      "2020-08-11\n",
      "2018-06-20\n",
      "2015-11-16\n",
      "2017-08-25\n",
      "2020-11-10\n",
      "2020-07-15\n",
      "2013-09-23\n",
      "2013-02-12\n",
      "2015-06-22\n",
      "2015-05-06\n",
      "2014-06-23\n",
      "2021-08-16\n",
      "2016-09-20\n",
      "2013-04-23\n",
      "2020-07-21\n",
      "2016-08-02\n",
      "2019-04-16\n",
      "2017-08-21\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "#make empty list to store the dates already sampled\n",
    "done_list = []\n",
    "\n",
    "#define important download parameters\n",
    "days = 7\n",
    "hour = 1 #which our of the day, impoetant: also change in the columnnames 2 times!!!)\n",
    "metadata['temp_01'] = '' #create new column to dstore the data #change also in last line before the else!!!\n",
    "\n",
    "#getting the data\n",
    "for idx,row in enumerate(metadata.index): #takes first element in metadata list\n",
    "    #print(row, idx)\n",
    "    start_date = metadata.date[idx]\n",
    "    date_index_list = metadata.index[metadata.date == start_date]#list with all the indexes of dates with the same date\n",
    "    #print(date_index_list)\n",
    "    if start_date not in done_list:#if data for the given date was already downloaded, this row will be skipped\n",
    "        temp_list = [[0] * days for i in range(len(date_index_list))]#make list of lists to store the values inside\n",
    "        done_list.append(start_date)#list of dates already samples\n",
    "        start_date = climf.get_start_date(start_date)#formate to time object\n",
    "        print(start_date)\n",
    "        for x in range(days):\n",
    "            count = 0\n",
    "            day_date = start_date - timedelta(days=x)\n",
    "            ds, stop = climf.get_ds_aws(day_date, hour)#getting the temperature array for the specified date\n",
    "            for index in date_index_list:\n",
    "                x_grid = metadata.x_grid[index]\n",
    "                y_grid = metadata.y_grid[index]\n",
    "                if stop == True:\n",
    "                    temp_list[count][x] = np.nan\n",
    "                else:\n",
    "                    temp_list[count][x] = ds[x_grid][y_grid]\n",
    "                count += 1\n",
    "                if x == days-1 and index == date_index_list[len(date_index_list)-1]:#if condition is met put the values in the metadata file\n",
    "                    for i in range(len(temp_list)):                    \n",
    "                        metadata.temp_01.loc[date_index_list[i]] = temp_list[i] #not index but  \n",
    "    else:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the download was interupted the code in the next cell can be used to continue with the download. Since the dates already dowloaded are saved in 'done_list', it can continue from that withouth starting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_list.pop()#remove last element (in case it didn work with that)\n",
    "len(done_list)#how many dates are already inside (1637 unique dates in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "\n",
    "temp_list = []\n",
    "#done_list = [] deactivate because we want to continue from the old donelist\n",
    "days = 7\n",
    "hour = 12 #which our of the day\n",
    "\n",
    "\n",
    "for idx,row in enumerate(metadata.index): #takes first element in metadata list\n",
    "    #print(row, idx)\n",
    "    start_date = metadata.date[idx]\n",
    "    date_index_list = metadata.index[metadata.date == start_date]#list with all the indexes of dates with the same date\n",
    "    #print(date_index_list)\n",
    "    if start_date not in done_list:#only do if this date wasnt used before\n",
    "        temp_list = [[0] * days for i in range(len(date_index_list))]#make list of lists to store the values inside\n",
    "        #print(len(temp_list), len(temp_list[0]))\n",
    "        done_list.append(start_date)#list of dates already samples\n",
    "        start_date = climf.get_start_date(start_date)#formate to time object\n",
    "        print(start_date)\n",
    "        for x in range(days):\n",
    "            count = 0\n",
    "            day_date = start_date - timedelta(days=x)\n",
    "            ds, stop = climf.get_ds_aws(day_date, hour)#getting the temperature array for the specified date\n",
    "            for index in date_index_list:\n",
    "                x_grid = metadata.x_grid[index]\n",
    "                y_grid = metadata.y_grid[index]\n",
    "                if stop == True:\n",
    "                    temp_list[count][x] = np.nan\n",
    "                else:\n",
    "                    temp_list[count][x] = ds[x_grid][y_grid]\n",
    "                count += 1\n",
    "                if x == days-1 and index == date_index_list[len(date_index_list)-1]:#if condition is met put the values in the metadata file\n",
    "                    for i in range(len(temp_list)):                    \n",
    "                        metadata.temp_01.loc[date_index_list[i]] = temp_list[i] #not index but  \n",
    "    else:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata.to_csv('../data/metadata_temp_12_01_06_complete.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad948ff7b48983452e55733fd71e7df6f615a0b1ed05e1cd9f8bab22d77c40aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
