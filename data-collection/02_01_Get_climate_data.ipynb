{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NOAA HRRR is a real-time 3km resolution, hourly updated, cloud-resolving, convection-allowing atmospheric model, initialized by 3km grids with 3km radar assimilation.\n",
    "\n",
    "This notebook provides an example of accessing HRRR data, including (1) finding the data file corresponding to a date and time, (2) retrieving a portion of that file from blob storage which includes the surface temperature variable, (3) opening the file using the xarray library, and (4) rendering an image of the forecast."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is documented at http://aka.ms/ai4edata-hrrr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import climatedata_functions as climf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"../data/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = climf.get_ds()#only words with recent dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make metadafile with gridpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranform coordinates for longitude\n",
    "metadata['longitude_trans'] = metadata['longitude']+360 \n",
    "#add columns for gridpoints\n",
    "metadata['x_grid'] = ''\n",
    "metadata['y_grid'] = ''\n",
    "\n",
    "#make new metadatafile and save as cvs \n",
    "#metadata_new = climf.save_grids(metadata.head(), ds) #(takes approx 6 minutes)\n",
    "#metadata_new.to_csv('../data/metadata_grids.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load new metadata file that includes the gridpoints\n",
    "metadata = pd.read_csv(\"../data/metadata_grids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>longitude_trans</th>\n",
       "      <th>x_grid</th>\n",
       "      <th>y_grid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aabm</td>\n",
       "      <td>39.080319</td>\n",
       "      <td>-86.430867</td>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>train</td>\n",
       "      <td>273.569133</td>\n",
       "      <td>570</td>\n",
       "      <td>1217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>aabn</td>\n",
       "      <td>36.559700</td>\n",
       "      <td>-121.510000</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>238.490000</td>\n",
       "      <td>550</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>aacd</td>\n",
       "      <td>35.875083</td>\n",
       "      <td>-78.878434</td>\n",
       "      <td>2020-11-19</td>\n",
       "      <td>train</td>\n",
       "      <td>281.121566</td>\n",
       "      <td>488</td>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   uid   latitude   longitude        date  split  \\\n",
       "0           0  aabm  39.080319  -86.430867  2018-05-14  train   \n",
       "1           1  aabn  36.559700 -121.510000  2016-08-31   test   \n",
       "2           2  aacd  35.875083  -78.878434  2020-11-19  train   \n",
       "\n",
       "   longitude_trans  x_grid  y_grid  \n",
       "0       273.569133     570    1217  \n",
       "1       238.490000     550     192  \n",
       "2       281.121566     488    1455  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get temperatures for gridpoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make function that gets the mean/median temperature for one date for all the places sampled at that date (to save time) and the previos x days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"../data/metadata_temp_12_01_06_complete_without_a.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-14\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "#make empty list to store the dates already sampled\n",
    "done_list = []\n",
    "\n",
    "#define important download parameters\n",
    "days = 7\n",
    "hour = 1 #which our of the day, impoetant: also change in the columnnames 2 times!!!)\n",
    "metadata['temp_01'] = '' #create new column to dstore the data #change also in last line before the else!!!\n",
    "\n",
    "#getting the data\n",
    "for idx,row in enumerate(metadata.index): #takes first element in metadata list\n",
    "    #print(row, idx)\n",
    "    start_date = metadata.date[idx]\n",
    "    date_index_list = metadata.index[metadata.date == start_date]#list with all the indexes of dates with the same date\n",
    "    #print(date_index_list)\n",
    "    if start_date not in done_list:#only do if this date wasnt used before\n",
    "        temp_list = [[0] * days for i in range(len(date_index_list))]#make list of lists to store the values inside\n",
    "        #print(len(temp_list), len(temp_list[0]))\n",
    "        done_list.append(start_date)#list of dates already samples\n",
    "        start_date = climf.get_start_date(start_date)#formate to time object\n",
    "        print(start_date)\n",
    "        for x in range(days):\n",
    "            count = 0\n",
    "            day_date = start_date - timedelta(days=x)\n",
    "            ds, stop = climf.get_ds_aws(day_date, hour)#getting the temperature array for the specified date\n",
    "            for index in date_index_list:\n",
    "                x_grid = metadata.x_grid[index]\n",
    "                y_grid = metadata.y_grid[index]\n",
    "                if stop == True:\n",
    "                    temp_list[count][x] = np.nan\n",
    "                else:\n",
    "                    temp_list[count][x] = ds[x_grid][y_grid]\n",
    "                count += 1\n",
    "                if x == days-1 and index == date_index_list[len(date_index_list)-1]:#if condition is met put the values in the metadata file\n",
    "                    for i in range(len(temp_list)):                    \n",
    "                        metadata.temp_01.loc[date_index_list[i]] = temp_list[i] #not index but  \n",
    "    else:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.to_csv('../data/metadata_temp_01_06_12_complete.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the download was interupted the code in the next cell can be used to continue with the download. Since the dates already dowloaded are saved in 'done_list', it can continue from that withouth starting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_list.pop()#remove last element (in case it didn work with that)\n",
    "len(done_list)#how many dates are already inside (1637 unique dates in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "\n",
    "temp_list = []\n",
    "#done_list = [] deactivate because we want to continue from the old donelist\n",
    "days = 7\n",
    "hour = 12 #which our of the day\n",
    "\n",
    "\n",
    "for idx,row in enumerate(metadata.index): #takes first element in metadata list\n",
    "    #print(row, idx)\n",
    "    start_date = metadata.date[idx]\n",
    "    date_index_list = metadata.index[metadata.date == start_date]#list with all the indexes of dates with the same date\n",
    "    #print(date_index_list)\n",
    "    if start_date not in done_list:#only do if this date wasnt used before\n",
    "        temp_list = [[0] * days for i in range(len(date_index_list))]#make list of lists to store the values inside\n",
    "        #print(len(temp_list), len(temp_list[0]))\n",
    "        done_list.append(start_date)#list of dates already samples\n",
    "        start_date = climf.get_start_date(start_date)#formate to time object\n",
    "        print(start_date)\n",
    "        for x in range(days):\n",
    "            count = 0\n",
    "            day_date = start_date - timedelta(days=x)\n",
    "            ds, stop = climf.get_ds_aws(day_date, hour)#getting the temperature array for the specified date\n",
    "            for index in date_index_list:\n",
    "                x_grid = metadata.x_grid[index]\n",
    "                y_grid = metadata.y_grid[index]\n",
    "                if stop == True:\n",
    "                    temp_list[count][x] = np.nan\n",
    "                else:\n",
    "                    temp_list[count][x] = ds[x_grid][y_grid]\n",
    "                count += 1\n",
    "                if x == days-1 and index == date_index_list[len(date_index_list)-1]:#if condition is met put the values in the metadata file\n",
    "                    for i in range(len(temp_list)):                    \n",
    "                        metadata.temp_01.loc[date_index_list[i]] = temp_list[i] #not index but  \n",
    "    else:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata.to_csv('../data/metadata_temp_12_01_06_complete.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad948ff7b48983452e55733fd71e7df6f615a0b1ed05e1cd9f8bab22d77c40aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
